{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Classification, Regression, Clustering & More"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of this notebook\n",
    "\n",
    "* Introduction to time series classification, regression, clustering\n",
    "* `sktime` data format fo \"time series panels\" = collections of time series\n",
    "* Basic vignettes for TSC, TSR, TSCl\n",
    "* Advanced vignettes - pipelines, ensembles, tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Deal with *collections of time series* = \"panel data\"\n",
    "\n",
    "Classification = try to assign one *category* per time series, after training on time series/category examples\n",
    "\n",
    "- Example: Daily energy consumption profile over time - Predict season, e.g., winter/summer, or type of consumer\n",
    "\n",
    "Regression = try to assign one continuous *numerical value* per time series, after training on time series/category examples\n",
    "\n",
    "- Example: Temperature/pressure/time profile of chemical reactor - Predict total purity (fraction of 1)\n",
    "\n",
    "Clustering = put different time series in a small number of similarity buckets\n",
    "\n",
    "- Example: Service Level Agreement (SLA) Breaches - Group the collected Panel data to identify common reasons for SLA failures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Series Classification:\n",
    "\n",
    "<img src=\"./img/tsc.png\" width=\"600\" alt=\"time series classification\"> [<i>&#x200B;</i>](./img/tsc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Increase display width\n",
    "pd.set_option(\"display.width\", 1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Panel data - `sktime` data formats <a name=\"panel\"></a>\n",
    "\n",
    "`Panel` is an abstract data type where the values are observed for:\n",
    "\n",
    "* `instance`, e.g., patient\n",
    "* `variable`, e.g., blood pressure, body temperature of the patient\n",
    "* `time`/`index`, e.g., January 12, 2023 (usually but not necessarily a time index!)\n",
    "\n",
    "One value X is: \"patient 'A' had blood pressure 'X' on January 12, 2023\"\n",
    "\n",
    "Time series classification, regression, clustering: slices `Panel` data by instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Preferred format 1: `pd.DataFrame` with 2-level `MultiIndex`, (instance, time) and columns: variables\n",
    "\n",
    "Preferred format 2: 3D `np.ndarray` with index (instance, variable, time)\n",
    "\n",
    "* `sktime` supports and recognizes multiple data formats for convenience and internal use, e.g., `dask`, `xarray`\n",
    "* abstract data type = \"scitype\"; in-memory specification = \"mtype\"\n",
    "* More information in tutorial on [in-memory data representations and data loading](https://www.sktime.net/en/latest/examples/AA_datatypes_and_datasets.html#In-memory-data-representations-and-data-loading)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Preferred format 1 - `pd-multiindex` specification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pd-multiindex` = `pd.DataFrame` with 2-level `MultiIndex`, (instance, time) and columns: variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.datasets import load_italy_power_demand\n",
    "\n",
    "# load an example time series panel in pd-multiindex mtype\n",
    "X, _ = load_italy_power_demand(return_type=\"pd-multiindex\")\n",
    "\n",
    "# renaming columns for illustrative purposes\n",
    "X.columns = [\"total_power_demand\"]\n",
    "X.index.names = [\"day_ID\", \"hour_of_day\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Italy power demand dataset has:\n",
    "\n",
    "* 1096 individual time series instances = single days of total power demand (mean subtracted)\n",
    "* one single variable per time series instances, `total_power_demand`\n",
    "    * total power demand on that day, in that hourly period\n",
    "    * Since there's only one column, it is a univariate dataset\n",
    "* individual time series are observed at 24 time (period) points (the same number for all instances)\n",
    "\n",
    "In the dataset, days are jumbled and of different scope (independent sampling).\n",
    "* considered independent - because `hour_of_day` in one sample doesn't affect `hour_of_day` in another\n",
    "* for task, e.g., \"identify season or weekday/weekend from pattern\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.datasets import load_basic_motions\n",
    "\n",
    "# load an example time series panel in pd-multiindex mtype\n",
    "X, _ = load_basic_motions(return_type=\"pd-multiindex\")\n",
    "\n",
    "# renaming columns for illustrative purposes\n",
    "X.columns = [\"accel_1\", \"accel_2\", \"accel_3\", \"gyro_1\", \"gyro_2\", \"gyro_3\"]\n",
    "X.index.names = [\"trial_no\", \"timepoint\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic motions dataset has:\n",
    "\n",
    "* 80 individual time series instances = trials = person engaging in an activity like running, badminton, etc.\n",
    "* six variables per time series instance, `dim_0` to `dim_5` (renamed according to the values they represent)\n",
    "    * 3 accelerometer and 3 gyrometer measurements\n",
    "    * hence a multivariate dataset\n",
    "* individual time series are observed at 100 time points (the same number for all instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The outermost index represents the instance number\n",
    "# whereas the inner index represents the index of the particular index\n",
    "# within that instance.\n",
    "X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas provides a simple way to access a range of value in the multi-indexed dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select:\n",
    "# * the fourth variable (gyroscope 1)\n",
    "# * of the first instance (trial 1 = 0 in python)\n",
    "# * values at all 100 timestamps\n",
    "#\n",
    "X.loc[0, \"gyro_1\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if you want to access the individual values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select:\n",
    "# * the fifth time time point (5 = 4 in python, because of 0-indexing)\n",
    "# * the third variable (accelerometer 3)\n",
    "# * of the forty-third instance (trial 43 = 42 in python)\n",
    "\n",
    "X.loc[(42, 4), \"accel_3\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 preferred format 2 - `numpy3D` specification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`numpy3D` = 3D `np.ndarray` with index (instance, variable, time)\n",
    "\n",
    "instance/time index is interpreted as integer\n",
    "\n",
    "IMPORTANT: unlike `pd-multiindex`, this assumes:\n",
    "\n",
    "* all individual series have the same length\n",
    "* all individual series have the same index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.datasets import load_italy_power_demand\n",
    "\n",
    "# load an example time series panel in numpy mtype\n",
    "X, _ = load_italy_power_demand(return_type=\"numpy3D\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Italy power demand dataset has:\n",
    "\n",
    "* 1096 individual time series instances = single days of total power demand (mean subtracted)\n",
    "* one single variable per time series instances, unnamed in numpy\n",
    "* individual time series are observed at 24 time (period) points (the same number for all instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (num_instances, num_variables, length)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.datasets import load_basic_motions\n",
    "\n",
    "# load an example time series panel in numpy mtype\n",
    "X, _ = load_basic_motions(return_type=\"numpy3D\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic motions dataset has:\n",
    "\n",
    "* 80 individual time series instances = trials = person engaging in activity (running, badminton, etc)\n",
    "* six variables per time series instance, unnamed in numpy\n",
    "* individual time series are observed at 100 time points (the same number for all instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Time Series Classification, Regression, Clustering - Basic Vignettes\n",
    "\n",
    "Above tasks are very similar to \"tabular\" classification, regression, clustering, as in `sklearn`\n",
    "\n",
    "Main distinction:\n",
    "* in \"tabular\" classification etc, one (feature) instance row vector of features\n",
    "* in TSC, one (feature) instance is a full time series, possibly unequal length, distinct index set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/tsc.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "More formally:\n",
    "\n",
    "* \"tabular\" classification:\n",
    "    * training pairs $(x_1, y_1), \\dots, (x_n, y_n)$\n",
    "        * where $x_i$ are rows of a `pd.DataFrame` (same col types)\n",
    "        * and $y_i \\in \\mathcal{C}$ for a finite set $\\mathcal{C}$\n",
    "    * is used to train a classifier that\n",
    "        * for a new `pd.DataFrame` row $x_*$\n",
    "        * predicts $y_* \\in \\mathcal{C}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* time series classification:\n",
    "    * training pairs $(x_1, y_1), \\dots, (x_n, y_n)$\n",
    "        * where $x_i$ are time series instances, from a certain domain\n",
    "        * and $y_i \\in \\mathcal{C}$ for a finite set $\\mathcal{C}$\n",
    "    * is used to train a classifier that\n",
    "        * for a new time series instance $x_*$\n",
    "        * predicts $y_* \\in \\mathcal{C}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "very similar for time series regression, clustering - exercise left to reader :-)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sktime` design implications:\n",
    "\n",
    "* need representation of collections of time series (panels),\n",
    "    see tutorial [In-memory data representations and data loading](AA_datatypes_and_datasets.ipynb) for more details on representation of Panel data.\n",
    "    * same as in \"adjacent\" learning tasks, e.g., panel forecasting\n",
    "    * same as for transformation estimators\n",
    "* algorithms that use sequentiality, can deal with unequal length, missing values etc \n",
    "* algorithms usually based on distances or kernels between time series - need to cover that in framework\n",
    "* but we can use familiar `fit` / `predict` and `scikit-learn` / `scikit-base` interface!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Time Series Classification - deployment vignette"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic deployment vignette for TSC:\n",
    "\n",
    "1. load/setup training data, `X` in a `Panel` (more specifically `numpy3D`) format, `y` as 1D `np.ndarray`\n",
    "2. load/setup new data for prediction (can be done after 3 too)\n",
    "3. specify the classifier using `sklearn`-like syntax\n",
    "4. fit classifier to training data, `fit(X, y)`\n",
    "5. predict labels on new data, `predict(X_new)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps 1, 2 - prepare osuleaf dataset (train and new)\n",
    "from sktime.datasets import load_italy_power_demand\n",
    "\n",
    "X_train, y_train = load_italy_power_demand(split=\"train\", return_type=\"numpy3D\")\n",
    "X_new, _ = load_italy_power_demand(split=\"test\", return_type=\"numpy3D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is in numpy3D format, but could also be pd-multiindex or other\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y is a 1D np.ndarray of labels - same length as number of instances in X_train\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3 - specify the classifier\n",
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "\n",
    "# example 1 - 3-NN with simple dynamic time warping distance (requires numba)\n",
    "clf = KNeighborsTimeSeriesClassifier(n_neighbors=3)\n",
    "\n",
    "# example 2 - custom distance:\n",
    "# 3-nearest neighbour classifier with Euclidean distance (on flattened time series)\n",
    "# (requires scipy)\n",
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "from sktime.dists_kernels import FlatDist, ScipyDist\n",
    "\n",
    "eucl_dist = FlatDist(ScipyDist())\n",
    "clf = KNeighborsTimeSeriesClassifier(n_neighbors=3, distance=eucl_dist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we could specify any `sktime` classifier here - the rest remains the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all classifiers is scikit-learn / scikit-base compatible!\n",
    "# nested parameter interface via get_params, set_params\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4 - fit/train the classifier\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the classifier is now fitted\n",
    "clf.is_fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we can inspect fitted parameters if we like\n",
    "clf.get_fitted_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 5 - predict labels on new data\n",
    "y_pred = clf.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred is an 1D np.ndarray, similar to sklearn classification output\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions and unique counts, for illustration\n",
    "unique, counts = np.unique(y_pred, return_counts=True)\n",
    "unique, counts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all together in one cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps 1, 2 - prepare osuleaf dataset (train and new)\n",
    "from sktime.datasets import load_italy_power_demand\n",
    "\n",
    "X_train, y_train = load_italy_power_demand(split=\"train\", return_type=\"numpy3D\")\n",
    "X_new, _ = load_italy_power_demand(split=\"test\", return_type=\"numpy3D\")\n",
    "\n",
    "# step 3 - specify the classifier\n",
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "from sktime.dists_kernels import FlatDist, ScipyDist\n",
    "\n",
    "eucl_dist = FlatDist(ScipyDist())\n",
    "clf = KNeighborsTimeSeriesClassifier(n_neighbors=3, distance=eucl_dist)\n",
    "\n",
    "# step 4 - fit/train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# step 5 - predict labels on new data\n",
    "y_pred = clf.predict(X_new)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Time Series Classification - simple evaluation vignette"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation is similar to `sklearn` classifiers - we split a dataset and evaluate performance on the test set.\n",
    "\n",
    "This includes as additional steps:\n",
    "\n",
    "* splitting the initial, historical data, e.g., using `train_test_split`\n",
    "* comparing predictions with a held out data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "from sktime.datasets import load_italy_power_demand\n",
    "\n",
    "# data should be split into train/test\n",
    "X_train, y_train = load_italy_power_demand(split=\"train\", return_type=\"numpy3D\")\n",
    "X_test, y_test = load_italy_power_demand(split=\"test\", return_type=\"numpy3D\")\n",
    "\n",
    "# step 3-5 are the same\n",
    "\n",
    "from sktime.dists_kernels import FlatDist, ScipyDist\n",
    "\n",
    "eucl_dist = FlatDist(ScipyDist())\n",
    "clf = KNeighborsTimeSeriesClassifier(n_neighbors=3, distance=eucl_dist)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# for simplest evaluation, compare ground truth to predictions\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 Multivariate Time Series Classification - basic vignettes\n",
    "\n",
    "In time series classification, handling multivariate data can be crucial when each variable or feature impacts classification. This section introduces three methods to deal with multivariate data.\n",
    "\n",
    "1. Classifiers with Native Multivariate Data support: Some estimators natively support multivaridate data. An example \n",
    "of this is `TimeSeriesForestClassifier`. This capability is also indicated by `capability: multivariate` tag. Refer [Section 2.3](#23-searching-for-estimators-estimator-tags) for more.\n",
    "2. `IndepDist`: Aggregates univariate distances across multiple variables to calculate an overall distance for multivariate data. Mathematically, let $d(x_i​,y_i​)$ be a univariate distance function applied to a variable i. The aggregated multivariate distance $d_y(x,y)$ can be defined as $$d_g​(x,y)=g(d(x_1​,y_1​),d(x_2​,y_2​),…,d(x_D​,y_D​))$$ where g is an aggregation function (eg. sum, mean). This approach is beneficial when each variable provides unique information for classification. Refer this [link](https://www.sktime.net/en/latest/api_reference/auto_generated/sktime.dists_kernels.indep.IndepDist.html) for a detailed walkthrough.\n",
    "\n",
    "3. `CombinedDistance`: This class enables combining multiple univariate distances or kernel transformers through an arithmetic operation (e.g addition, multiplication) on their respective distance matrices. Suppose we have N base distance matrices $dist_1$, $dist_2$, $dist_3$,..., $dist_4$ calculated from different distance transformers applied to a dataset. For an operation $f$ (eg. `np.sum` or `np.mean`) the `CombinedDistance` outputs a single matrix $dist$ where each entry is calculated as $$dist[i,j] = f(dist_1​[i,j],dist_2​[i,j],…,dist_N​[i,j])$$ This formula provides flexibility to test out various transformations. Refer this [link](https://www.sktime.net/en/latest/api_reference/auto_generated/sktime.dists_kernels.algebra.CombinedDistance.html) for a detailed walkthrough.\n",
    "\n",
    "For further details on distance methods, refer to [Tutorial](https://www.sktime.net/en/latest/examples/06_distances_kernels_alignment.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TimeSeriesForestClassifier` is a good example of an estimator which handles multivariate data natively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
    "from sktime.datasets import load_arrow_head\n",
    "\n",
    "# step 1-- prepare a dataset (multivariate for demonstration)\n",
    "X_train, y_train = load_arrow_head(split=\"train\")\n",
    "X_new, _ = load_arrow_head(split=\"test\")\n",
    "\n",
    "# step 2-- define the TimeSeriesForestClassifier\n",
    "tsf = TimeSeriesForestClassifier()\n",
    "\n",
    "# step 3-- train the classifier on the data\n",
    "tsf.fit(X_train, y_train)\n",
    "\n",
    "# step 4-- predict labels on the new data\n",
    "y_pred = tsf.predict(X_new[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of `IndepDist` with `sum` Aggregation Function on a Multivariate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "from sktime.datasets import load_arrow_head\n",
    "from sktime.dists_kernels.compose_tab_to_panel import FlatDist\n",
    "from sktime.dists_kernels.indep import IndepDist\n",
    "from sktime.dists_kernels.scipy_dist import ScipyDist\n",
    "\n",
    "# step 1-- prepare a dataset (multivariate for demonstrating purposes)\n",
    "X_train, y_train = load_arrow_head(split=\"train\", return_X_y=True)\n",
    "X_new, _ = load_arrow_head(split=\"test\", return_X_y=True)\n",
    "\n",
    "# step 2-- define a classification estimator with distance parameter IndepDist\n",
    "indep_dist_sum = IndepDist(dist=FlatDist(ScipyDist()), aggfun=\"sum\")\n",
    "knn_sum = KNeighborsTimeSeriesClassifier(distance=indep_dist_sum)\n",
    "\n",
    "# step 3-- train the models with the data\n",
    "knn_sum.fit(X_train, y_train)\n",
    "\n",
    "# step 4-- predict labels on new data\n",
    "y_pred = knn_sum.predict(X_new[:5])  # Smaller set for faster runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`IndepDist` provides the choice of being able to select `aggfun` parameter out of the many choices - `sum`, `mean`, `median`, `max`, `min`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prediction with sum aggregation: \", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is another implementation using `CombinedDistance` which applies `DtwDist()` on separate components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "from sktime.datasets import load_arrow_head\n",
    "from sktime.dists_kernels.algebra import CombinedDistance\n",
    "from sktime.dists_kernels.compose_tab_to_panel import AggrDist, FlatDist\n",
    "from sktime.dists_kernels.scipy_dist import ScipyDist\n",
    "\n",
    "# step 1-- load a dataset\n",
    "X_train, y_train = load_arrow_head(split=\"train\", return_X_y=True)\n",
    "X_new, _ = load_arrow_head(split=\"test\", return_X_y=True)\n",
    "\n",
    "# step 2-- setup combined distance\n",
    "combined_dist = CombinedDistance([FlatDist(ScipyDist()), AggrDist(ScipyDist())])\n",
    "\n",
    "## step 3-- initialize the classifier\n",
    "knn_combined_dist = KNeighborsTimeSeriesClassifier(distance=combined_dist)\n",
    "\n",
    "## step 4-- train the classifier\n",
    "knn_combined_dist.fit(X_train, y_train)\n",
    "\n",
    "## step 5-- obtain the predictions from the classifier.\n",
    "y_pred = knn_combined_dist.predict(X_new[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.6 Time Series Regression - basic vignettes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TSR vignettes are exactly the same as TSC, except that:\n",
    "\n",
    "* `y` in `fit` input and `predict` output should be float 1D `np.ndarray`, not categorical\n",
    "* other algorithms are commonly used and/or performant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps 1, 2 - prepare dataset (train and new)\n",
    "from sktime.datasets import load_covid_3month\n",
    "\n",
    "X_train, y_train = load_covid_3month(split=\"train\")\n",
    "y_train = y_train.astype(\"float\")\n",
    "X_new, _ = load_covid_3month(split=\"test\")\n",
    "X_new = X_new.loc[:2]  # smaller dataset for faster notebook runtime\n",
    "\n",
    "# step 3 - specify the regressor\n",
    "from sktime.regression.distance_based import KNeighborsTimeSeriesRegressor\n",
    "\n",
    "clf = KNeighborsTimeSeriesRegressor(n_neighbors=3, distance=eucl_dist)\n",
    "\n",
    "# step 4 - fit/train the regressor\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# step 5 - predict labels on new data\n",
    "y_pred = clf.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred  # predictions are array of float"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.7 Time Series Clustering - basic vignettes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TS clustering is similar - 1st step is also `fit`, but unsupervised\n",
    "\n",
    "i.e., no labels `y`, and next step is inspecting clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 - prepare dataset (train and new)\n",
    "from sktime.datasets import load_italy_power_demand\n",
    "\n",
    "X, _ = load_italy_power_demand(split=\"train\", return_type=\"numpy3D\")\n",
    "\n",
    "# step 2 - specify the clusterer\n",
    "from sktime.clustering.dbscan import TimeSeriesDBSCAN\n",
    "from sktime.dists_kernels import FlatDist, ScipyDist\n",
    "\n",
    "eucl_dist = FlatDist(ScipyDist())\n",
    "clst = TimeSeriesDBSCAN(distance=eucl_dist, eps=2)\n",
    "\n",
    "# step 3 - fit the clusterer to the data\n",
    "clst.fit(X)\n",
    "\n",
    "# step 4 - inspect the clustering\n",
    "clst.get_fitted_params()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Searching for estimators, estimator tags"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimators in `sktime` are tagged.\n",
    "\n",
    "Tags starting with \"capability\" indicate things the estimator can or cannot do, e.g.,\n",
    "\n",
    "* `\"capability:missing_values\"` - dealing with missing values\n",
    "* `\"capability:multivariate\"` - dealing with multivariate input\n",
    "* `\"capability:unequal_length\"` - deaing with time series panels where the individual time series have unequal length and/or unequal index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all tags for an estimator scitype (e.g., classifier, regressor) can be inspected by `sktime.registry.all_tags`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.registry import all_tags\n",
    "\n",
    "all_tags(\"classifier\", as_dataframe=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "valid estimator types are listed in the `all_tags` docstring, or `sktime.registry.BASE_CLASS_REGISTER`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.registry import get_obj_scitype_list\n",
    "\n",
    "# get only fist table column, the list of types\n",
    "get_obj_scitype_list()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to find all estimators of a certain type, use `sktime.registry.all_estimators`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all classifiers in sktime\n",
    "from sktime.registry import all_estimators\n",
    "\n",
    "all_estimators(\"classifier\", as_dataframe=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for listing all estimators of a certain type with a certain capability,\n",
    "use the `filter_tags` argument of `all_estimators`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all classifiers in sktime\n",
    "# that can classify panels of time series containing missing data\n",
    "from sktime.registry import all_estimators\n",
    "\n",
    "all_estimators(\n",
    "    \"classifier\",\n",
    "    as_dataframe=True,\n",
    "    filter_tags={\"capability:missing_values\": True},\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "side note:\n",
    "\n",
    "don't worry about how short the list is - when in doubt, it is always possible to pipeline with `Imputer`\n",
    "\n",
    "as in the next section :-)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Pipelines, Feature Extraction, Tuning, Composition\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similar to `sklearn` for \"tabular\" classification, regression, etc,\n",
    "\n",
    "`sktime` has a rich set of tools for:\n",
    "\n",
    "* feature extraction via transformers\n",
    "* pipeline transformers with any estimator\n",
    "* tuning individual estimators or pipelines via grid search and similar\n",
    "* building ensembles out of individual estimators, or other composites\n",
    "\n",
    "`sktime` is also fully interoperable with `sklearn` interface if `numpy` based data mtypes are used\n",
    "\n",
    "(although this loses support for unequal length time series)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Primer on `sktime` transformers for feature extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all `sktime` transformers work natively with panel data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.datasets import load_italy_power_demand\n",
    "from sktime.transformations.series.detrend import Detrender\n",
    "\n",
    "# load some panel data\n",
    "X, _ = load_italy_power_demand(return_type=\"pd-multiindex\")\n",
    "\n",
    "# specify a linear detrender\n",
    "detrender = Detrender()\n",
    "\n",
    "# detrend X by removing linear trend from each instance\n",
    "X_detrended = detrender.fit_transform(X)\n",
    "X_detrended"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for panel tasks such as TSC, TSR, clustering, there are two distinctions to be aware of:\n",
    "\n",
    "* series-to-series transformers transform individual series to series, panels to panels. E.g., instance-wise detrender above\n",
    "* series-to-primitive transformers transform individual series to a set of tabular features. E>g., summary feature extractor\n",
    "\n",
    "either type of transform can be instance-wise:\n",
    "\n",
    "* instance-wise transforms use only the i-th series to transform the i-th series. E.g., instance-wise detrender\n",
    "* non-instance-wise transforms train on all series to transform the i-th series. E.g., PCA, overall mean detrender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a series-to-primitive transformer\n",
    "from sktime.transformations.series.summarize import SummaryTransformer\n",
    "\n",
    "# specify summary transformer\n",
    "summary_trafo = SummaryTransformer()\n",
    "\n",
    "# extract summary features - one per instance in the panel\n",
    "X_summaries = summary_trafo.fit_transform(X)\n",
    "X_summaries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just like classifiers, we can search for transformers of either type via the right tag:\n",
    "\n",
    "* `\"scitype:transform-input\"` and `\"scitype:transform-output\"` define input and output, e.g., \"series-to-series\" (both are scitype strings)\n",
    "* `\"scitype:instancewise\"` is boolean and tells us whether the transform is instance-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: looking for all series-to-primitive transformers that are instance-wise\n",
    "from sktime.registry import all_estimators\n",
    "\n",
    "all_estimators(\n",
    "    \"transformer\",\n",
    "    as_dataframe=True,\n",
    "    filter_tags={\n",
    "        \"scitype:transform-input\": \"Series\",\n",
    "        \"scitype:transform-output\": \"Primitives\",\n",
    "        \"scitype:instancewise\": True,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further details on transformations and feature extraction can be found in the tutorial 3, transformers.\n",
    "\n",
    "All composition steps therein (e.g., chaining, column subsetting) work together with all estimator types in `sktime`, including classifiers, regressors, clusterers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Pipelines for time series panel tasks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all panel estimators pipeline with `sktime` transformers, via the `*` dunder or `make_pipeline`.\n",
    "\n",
    "The pipeline does the following:\n",
    "\n",
    "* in `fit`: runs the transformers' `fit_transform` in sequence, then `fit` of the panel estimator\n",
    "* in `predict`, runs the fitted transformers' `transform` in sequence, then `predict` of the panel estimator\n",
    "\n",
    "(the logic is same as for `sklearn` pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "from sktime.transformations.series.exponent import ExponentTransformer\n",
    "\n",
    "pipe = ExponentTransformer() * KNeighborsTimeSeriesClassifier()\n",
    "\n",
    "# this constructs a ClassifierPipeline, which is also a classifier\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative to construct:\n",
    "from sktime.pipeline import make_pipeline\n",
    "\n",
    "pipe = make_pipeline(ExponentTransformer(), KNeighborsTimeSeriesClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.datasets import load_unit_test\n",
    "\n",
    "X_train, y_train = load_unit_test(split=\"TRAIN\")\n",
    "X_test, _ = load_unit_test(split=\"TEST\")\n",
    "\n",
    "# this is a ClassifierPipeline with the same interface as knn-classifier\n",
    "# first applies exponent transform, then knn-classifier\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sktime` transformers pipeline with `sklearn` classifiers!\n",
    "\n",
    "This allows to build \"time series feature extraction then `sklearn` classify`\" pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sktime.transformations.series.summarize import SummaryTransformer\n",
    "\n",
    "# specify summary transformer\n",
    "summary_rf = SummaryTransformer() * RandomForestClassifier()\n",
    "\n",
    "summary_rf.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Using transformers to deal with unequal length or missing values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pro tip: useful transformers to pipeline are those that \"improve\" capabilities!\n",
    "\n",
    "Search for these transformer tags:\n",
    "\n",
    "* `\"capability:unequal_length:removes\"` - ensures all instances in the panel have equal length afterwards. Examples: padding, cutting, resampling.\n",
    "* `\"capability:missing_values:removes\"` - removes all missing values from the data (e.g., series, panel) passed to it. Example: mean imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all transformers that guarantee that the output is equal length and equal index\n",
    "from sktime.registry import all_estimators\n",
    "\n",
    "all_estimators(\n",
    "    \"transformer\",\n",
    "    as_dataframe=True,\n",
    "    filter_tags={\"capability:unequal_length:removes\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all transformers that guarantee the output has no missing values\n",
    "from sktime.registry import all_estimators\n",
    "\n",
    "all_estimators(\n",
    "    \"transformer\",\n",
    "    as_dataframe=True,\n",
    "    filter_tags={\"capability:missing_values:removes\": True},\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "minor note:\n",
    "\n",
    "some transformers guarantee \"no missing values\" under some conditions but not always, e.g., `TimeBinAggregate`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's check the tags in one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all classifiers in sktime\n",
    "from sktime.classification.feature_based import SummaryClassifier\n",
    "\n",
    "no_missing_clf = SummaryClassifier()\n",
    "\n",
    "no_missing_clf.get_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.series.impute import Imputer\n",
    "\n",
    "clf_can_do_missing = Imputer() * SummaryClassifier()\n",
    "\n",
    "clf_can_do_missing.get_tags()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4 Tuning and model selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sktime` classifiers are compatible with `sklearn` model selection and composition tools using `sktime` data formats.\n",
    "\n",
    "This extends to grid tuning and cross-validation, as long as `numpy` based formats or length/instance indexed formats are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.datasets import load_unit_test\n",
    "\n",
    "X_train, y_train = load_unit_test(split=\"TRAIN\")\n",
    "X_test, _ = load_unit_test(split=\"TEST\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation using the `sklearn` `cross_val_score` and `KFold` functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "from sktime.classification.feature_based import SummaryClassifier\n",
    "\n",
    "clf = SummaryClassifier()\n",
    "\n",
    "cross_val_score(clf, X_train, y=y_train, cv=KFold(n_splits=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter tuning using `sklearn` `GridSearchCV`, we tune the _k_ and distance measure for a K-NN classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "\n",
    "knn = KNeighborsTimeSeriesClassifier()\n",
    "param_grid = {\"n_neighbors\": [1, 5], \"distance\": [\"euclidean\", \"dtw\"]}\n",
    "parameter_tuning_method = GridSearchCV(knn, param_grid, cv=KFold(n_splits=4))\n",
    "\n",
    "parameter_tuning_method.fit(X_train, y_train)\n",
    "y_pred = parameter_tuning_method.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.5 Advanced Composition cheat sheet - AutoML, bagging, ensembles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* common ensembling patterns: `BaggingClassifier`, `WeightedEnsembleClassifier`\n",
    "* composability with `sklearn` classifier, regressor building blocks still applies\n",
    "* AutoML can be achieved by combining tuning with `MultiplexClassifier` or `MultiplexTransformer`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pro tip: bagging with a fixed single column subset can be used to turn an univariate classifier into a multivariate classifier!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Appendix - Extension guide"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sktime` is meant to be easily extensible, for direct contribution to `sktime` as well as for local/private extension with custom methods.\n",
    "\n",
    "To extend `sktime` with a new local or contributed estimator, a good workflow to follow is:\n",
    "\n",
    "0. find the right extension template for the type of estimator you want to add - e.g., classifier, regressor, clusterer, etc. The extension templates are located in the [`extension_templates](https://github.com/sktime/sktime/blob/main/extension_templates) directory\n",
    "1. read through the extension template - this is a `python` file with `todo` blocks that mark the places in which changes need to be added.\n",
    "2. optionally, if you are planning any major surgeries to the interface: look at the base class - note that \"ordinary\" extension (e.g., new algorithm) should be easily doable without this.\n",
    "3. copy the extension template to a local folder in your own repository (local/private extension), or to a suitable location in your clone of the `sktime` or affiliated repository (if contributed extension), inside `sktime.[name_of_task]`; rename the file and update the file docstring appropriately.\n",
    "4. address the \"todo\" parts. Usually, this means: changing the name of the class, setting the tag values, specifying hyper-parameters, filling in `__init__`, `_fit`, `_predict` and/or other methods (for details see the extension template). You can add private methods as long as they do not override the default public interface. For more details, see the extension template.\n",
    "5. to test your estimator manually: import your estimator and run it in the basic vignettes above.\n",
    "6. to test your estimator automatically: call `sktime.tests.test_all_estimators.check_estimator` on your estimator. You can call this on a class or object instance. Ensure you have specified test parameters in the `get_test_params` method, according to the extension template.\n",
    "\n",
    "In case of direct contribution to `sktime` or one of its affiliated packages, additionally:\n",
    "* Add yourself as an author and/or a maintainer for the new estimator file(s), via `\"authors\"` and `\"maintainers\"` tag.\n",
    "* create a pull request that contains only the new estimators (and their inheritance tree, if it's not just one class), as well as the automated tests as described above.\n",
    "* in the pull request, describe the estimator and optimally provide a publication or other technical reference for the strategy it implements.\n",
    "* before making the pull request, ensure that you have all necessary permissions to contribute the code to a permissive license (BSD-3) open source project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sktime-skpro-skbase-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
